<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>【神经网络】通过代码学习Tensorflow6 | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【神经网络】通过代码学习Tensorflow6</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">【神经网络】通过代码学习Tensorflow6</h1><div class="post-meta"><a href="/2017/08/11/Tensorflow6/#comments" class="comment-count"></a><p><span class="date">Aug 11, 2017</span><span><a href="/categories/机器学习笔记/" class="category">机器学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>Tensorflow 高阶内容<br>使用 Tensorflow 搭建自编码器(Autoencoder)<br><a id="more"></a></p>
<h1 id="自编码器-Autoencoders"><a href="#自编码器-Autoencoders" class="headerlink" title="自编码器 Autoencoders"></a>自编码器 Autoencoders</h1><p>阅读本文前，务必详细阅读 <a href="http://www.cnblogs.com/caocan702/p/5665972.html" target="_blank" rel="external">常用模型之自编码器</a> 以及 <a href="https://www.zhihu.com/question/41490383" target="_blank" rel="external">知乎：为什么稀疏自编码器很少见到多层的？</a> 两篇文章，可以详细了解有关于 自编码器 的相关知识。以下只做部分摘抄。</p>
<p>自编码器可以理解为一个试图去还原其原始输入的系统。如下图所示。</p>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-064339.jpg" alt=""></p>
<p>对于自编码器，我们往往并不关心输出是啥（反正只是复现输入），<strong>我们真正关心的是中间层的编码，或者说是从输入到编码的映射。</strong></p>
<p>可以这么想，在我们强迫编码y和输入x不同的情况下，系统还能够去复原原始信号x，那么说明编码y已经承载了原始数据的所有信息，但以一种不同的形式！这就是 <strong>特征提取</strong> 啊，而且是自动学出来的！实际上，<strong>自动学习原始数据的特征表达也是神经网络和深度学习的核心目的之一。</strong></p>
<h2 id="自编码器与神经网络"><a href="#自编码器与神经网络" class="headerlink" title="自编码器与神经网络"></a>自编码器与神经网络</h2><p>简单来讲，神经网络就是在对原始信号逐层地做非线性变换，如下图所示。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-064928.jpg" width="40%"><br>该网络把输入层数据x∈Rn转换到中间层（隐层）h∈Rp，再转换到输出层y∈Rm。图中的每个节点代表数据的一个维度（偏置项图中未标出）。每两层之间的变换都是 <strong>“线性变化”+“非线性激活”</strong>。</p>
<p><strong>神经网络往往用于分类，其目的是去逼近从输入层到输出层的变换函数</strong>。因此，我们会定义一个目标函数来衡量当前的输出和真实结果的差异，利用该函数去逐步调整（如梯度下降）系统的参数，以使得整个网络尽可能去拟合训练数据。如果有正则约束的话，还同时要求模型尽量简单（防止过拟合）。</p>
<p>而自编码器试图复现其原始输入，因此，在训练中，网络中的输出应与输入相同，即y=x，因此，一个自编码器的输入、输出应有相同的结构，即<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-065303.jpg" width="40%"><br>我们利用训练数据训练这个网络，等训练结束后，这个网络即学习出了x→h→x的能力。对我们来说，此时的h是至关重要的，因为 <strong>它是在尽量不损失信息量的情况下，对原始数据的另一种表达。</strong></p>
<p>为了尽量学到有意义的表达，我们会给隐层加入一定的约束。从数据维度来看，常见以下两种情况：<br><strong>1. n &gt; p，即隐层维度小于输入数据维度。</strong><br>也就是说从x→h的变换是一种降维的操作，<strong>网络试图以更小的维度去描述原始数据而尽量不损失数据信息。</strong>实际上，当每两层之间的变换均为线性，且监督训练的误差是二次型误差时，该网络等价于PCA！没反应过来的童鞋可以反思下PCA是在做什么事情。<br><strong>2. n &lt; p，即隐层维度大于输入数据维度。</strong><br>这又有什么用呢？其实不好说，但比如我们同时约束h的表达尽量稀疏（有大量维度为0，未被激活），此时的编码器便是大名鼎鼎的 <strong>“稀疏自编码器”</strong> 。可为什么稀疏的表达就是好的？这就说来话长了，有人试图从人脑机理对比，即人类神经系统在某一刺激下，大部分神经元是被抑制的。个人觉得，从特征的角度来看更直观些，稀疏的表达意味着系统在尝试去特征选择，找出大量维度中真正重要的若干维。</p>
<h2 id="堆叠自编码器"><a href="#堆叠自编码器" class="headerlink" title="堆叠自编码器"></a>堆叠自编码器</h2><p>有过深度学习基础的童鞋想必了解，深层网络的威力在于其能够逐层地学习原始数据的多种表达。每一层的都以底一层的表达为基础，但往往更抽象，更加适合复杂的分类等任务。</p>
<p>堆叠自编码器实际上就在做这样的事情，如前所述，单个自编码器通过虚构x→h→x的三层网络，能够学习出一种特征变化h=fθ(x)（这里用θ表示变换的参数，包括W,b和激活函数）。实际上，当训练结束后，输出层已经没什么意义了，我们一般将其去掉，即将自编码器表示为<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-065803.jpg" width="40%"><br>之前之所以将自编码器模型表示为3层的神经网络，那是因为训练的需要，我们将原始数据作为假想的目标输出，以此构建监督误差来训练整个网络。等训练结束后，输出层就可以去掉了，我们关心的只是从x到h的变换。</p>
<p>接下来的思路就很自然了——我们已经得到特征表达h，那么我们可不可以将h再当做原始信息，训练一个新的自编码器，得到新的特征表达呢？当然可以！这就是所谓的堆叠自编码器（Stacked Auto-Encoder, SAE）。Stacked就是逐层垒叠的意思。</p>
<p>当把多个自编码器Stack起来之后，这个系统看起来就像这样：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-070011.jpg" width="90%"><br>需要注意的是，整个网络的训练不是一蹴而就的，而是逐层进行。按题主提到的结构n,m,k结构，实际上我们是先训练网络n→m→n，得到n→m的变换，然后再训练m→k→m，得到m→k的变换。最终堆叠成SAE，即为n→m→k的结果，整个过程就像一层层往上盖房子，这便是大名鼎鼎的<strong>layer-wise unsuperwised pre-training（逐层非监督预训练）</strong>，正是导致深度学习（神经网络）在2006年第3次兴起的核心技术。</p>
<h2 id="自编码器的变种形式"><a href="#自编码器的变种形式" class="headerlink" title="自编码器的变种形式"></a>自编码器的变种形式</h2><p>上述介绍的自编码器是最基本的形式。善于思考的童鞋可能已经意识到了这个问题：隐层的维度到底怎么确定？为什么稀疏的特征比较好？或者更准确的说，怎么才能称得上是一个好的表达（What defines a good representation）？<br>事实上，这个问题回答并不唯一，也正是从不同的角度去思考这个问题，导致了自编码器的各种变种形式出现。目前常见的几种模型总结如下<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-070227.jpg" width="80%"></p>
<h3 id="稀疏自编码器"><a href="#稀疏自编码器" class="headerlink" title="稀疏自编码器"></a>稀疏自编码器</h3><p>如前所示，这种模型背后的思想是，高维而稀疏的表达是好的。一般而言，我们不会指定隐层表达h中哪些节点是被抑制的（对于sigmoid单元即输出为0），而是指定一个<strong>稀疏性参数ρ</strong>，代表隐藏神经元的平均活跃程度（在训练集上取平均）。比如，当ρ=0.05时，可以认为隐层节点在95%的时间里都是被抑制的，只有5%的机会被激活。实际上，为了满足这一条件，隐层神经元的活跃度需要接近于0。</p>
<p>那么，怎么从数学模型上做到这点呢？思路也不复杂，既然要求平均激活度为ρ，那么只要引入一个度量，来衡量神经元i的实际激活度$\tilde{\rho_{a}}$ 与期望激活度ρ之间的差异即可，然后将这个度量添加到目标函数作为正则，训练整个网络即可。那么，什么样的度量适合这个任务呢？有过概率论、信息论基础的同学应该很容易想到它——相对熵，也就是KL散度（KL divergence）。</p>
<h3 id="降噪自编码器"><a href="#降噪自编码器" class="headerlink" title="降噪自编码器"></a>降噪自编码器</h3><p>DAE 的核心思想是，一个能够从中恢复出原始信号的表达未必是最好的，能够对“被污染/破坏”的原始数据编码、解码，然后还能恢复真正的原始数据，这样的特征才是好的。</p>
<p>稍微数学一点，假设 <code>原始数据</code> 被我们“故意破坏”，比如加入高斯白噪，或者把某些维度数据抹掉，将其作为 <code>输入数据</code> ，然后再对 <code>输入数据</code> 编码、解码，得到 <code>恢复信号</code>，该 <code>恢复信号</code> 尽可能逼近未被污染的 <code>原始数据</code>。</p>
<p>直观上理解，DAE希望学到的特征变换尽可能鲁棒，能够在一定程度上对抗原始数据的污染、缺失。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-11-070956.jpg" width="80%"></p>
<hr>
<p><strong>参考资料</strong><br><a href="http://www.cnblogs.com/caocan702/p/5665972.html" target="_blank" rel="external">常用模型之自编码器</a><br><a href="https://www.zhihu.com/question/41490383" target="_blank" rel="external">知乎：为什么稀疏自编码器很少见到多层的？</a><br><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">神经网络：Tensorflow</a></p>
</div><div class="tags"><a href="/tags/Autoencoder/">Autoencoder</a><a href="/tags/神经网络/">神经网络</a><a href="/tags/tensorflow/">tensorflow</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/08/11/Deep-Learning/" class="next">【深度学习】深度神经网络入门</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#自编码器-Autoencoders"><span class="toc-text">自编码器 Autoencoders</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#自编码器与神经网络"><span class="toc-text">自编码器与神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#堆叠自编码器"><span class="toc-text">堆叠自编码器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自编码器的变种形式"><span class="toc-text">自编码器的变种形式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#稀疏自编码器"><span class="toc-text">稀疏自编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#降噪自编码器"><span class="toc-text">降噪自编码器</span></a></li></ol></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/11/Tensorflow6/">【神经网络】通过代码学习Tensorflow6</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/11/Deep-Learning/">【深度学习】深度神经网络入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/有趣的机器学习/">【机器学习】有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/10/Tensorflow5/">【神经网络】通过代码学习Tensorflow5</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/05/Tensorflow4/">【神经网络】通过代码学习Tensorflow4</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/RBM/" style="font-size: 15px;">RBM</a> <a href="/tags/DBN/" style="font-size: 15px;">DBN</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Autoencoder/" style="font-size: 15px;">Autoencoder</a> <a href="/tags/RNTN/" style="font-size: 15px;">RNTN</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/dropout/" style="font-size: 15px;">dropout</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>