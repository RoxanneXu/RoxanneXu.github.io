<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>【深度学习】深度神经网络入门 | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【深度学习】深度神经网络入门</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">【深度学习】深度神经网络入门</h1><div class="post-meta"><a href="/2017/08/11/Deep-Learning/#comments" class="comment-count"></a><p><span class="date">Aug 11, 2017</span><span><a href="/categories/深度学习笔记/" class="category">深度学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>本文以非常浅显的语言讲解了一些深度学习基础知识。</p>
<a id="more"></a>
<h1 id="不同的深度神经网络适合的任务"><a href="#不同的深度神经网络适合的任务" class="headerlink" title="不同的深度神经网络适合的任务"></a>不同的深度神经网络适合的任务</h1><p>首先你是要确定你是想要构建一个分类器，还是想要找出你数据中的模式。</p>
<ol>
<li>非监督学习，比如从无标签的数据中发掘模式<br>建议使用：<code>受限玻尔兹曼机RBM</code> 或者 <code>自编码器AE</code>。</li>
<li>监督学习：从有标签的数据中获取分类器<ul>
<li>文本处理任务：感情分析、语句分析、命名体识别 -&gt;&gt; <code>RNTN</code></li>
<li>对于所有需要处理字符级别的语言任务模型 -&gt;&gt; <code>Recurrent Net</code></li>
<li>图像识别 -&gt;&gt; <code>DBN</code> <code>Convolutional Net</code></li>
<li>物体识别 -&gt;&gt; <code>Convolutional Net</code> <code>RNTN</code></li>
<li>语音识别 -&gt;&gt; <code>Recurrent Net</code></li>
</ul>
</li>
</ol>
<p><code>DBN（深度信念网络）</code> 与 <code>MLP/RELU（有修正线性单元的多层感知机）</code> 都擅长分类任务。<br><code>Recurrent Net（递归神经网络）</code> 用于时间序列分析。</p>
<h1 id="深度神经网络遇到的困难"><a href="#深度神经网络遇到的困难" class="headerlink" title="深度神经网络遇到的困难"></a>深度神经网络遇到的困难</h1><p><strong>当我们试着用反向传播算法去训练深度神经网络时，会碰到<code>梯度消失</code>和<code>梯度爆炸</code>的问题，这就导致了训练非常耗时，并且准确率也不高。</strong></p>
<p>在我们的训练过程当中，我么会通过不断调整 <code>weight</code> 和 <code>bais</code> 来减小 <code>loss。</code><br>过程中需要利用 梯度 ，梯度 衡量了 <code>loss</code> 关于 <code>weight</code> 或者 <code>bais</code> 的变化速率。<br>梯度越大，训练越快，梯度越小，训练越慢。</p>
<p>2006年以前深度神经网络效果非常差，还比不过浅层网络和其他机器学习算法。<br>主要是因为：使用反向传播的方法训练神经网络，往往会出现<code>梯度消失</code>，使网络的训练变得非常耗时，最后的准确率也会变得非常低。每个节点的梯度都是之前所有节点梯度的乘积。如果梯度较小（是一个0~1之间的数），那么经过几层的传播，梯度就会变得非常小。</p>
<h1 id="如何克服梯度消失问题？"><a href="#如何克服梯度消失问题？" class="headerlink" title="如何克服梯度消失问题？"></a>如何克服梯度消失问题？</h1><h2 id="受限玻尔兹曼机-RBM"><a href="#受限玻尔兹曼机-RBM" class="headerlink" title="受限玻尔兹曼机 RBM"></a>受限玻尔兹曼机 RBM</h2><p>这个方法通过<strong>重构数据</strong>，可以自动发现我们数据中的<strong>内在模式</strong>。</p>
<p>RBM 是一个浅层的二层网络，它的第一层是<code>可见层</code>，第二层是<code>隐藏层</code>。<br>RBM 名字中有 受限 二字主要是因为同一层内两个节点共享了一个链接。<br>RBM 在数学上等价于一个双向翻译器，在前向传播的时候，RBM 获得 inputs ，然后把它翻译成一组能够编码 inputs 的数字；在反向的过程中，它把这组数字重构成 inputs 。训练过程中 weight 和 bais 非常重要，可以告诉我们哪些特征是非常重要的。</p>
<p>过程：</p>
<ol>
<li>在前向传播的过程中，每一个输入都会和一个单独的 weight 和一个公共的 bais 组合，结果传给<code>隐藏层</code>，隐藏层中的节点有的被激活，有的没激活。</li>
<li>在反向传播的过程中，每一个激活值会和一个单独的 weight 和一个公共的 bais 结合，结果传回到<code>可见层</code>。</li>
<li>在可见层，重构出来值会跟原来的 inputs 做对比，使用 <code>KL离散值</code> 来衡量结果的质量。<br>上述1~3过程会一直重复，不断调整 weight 和 bais ，直到重构值和inputs尽可能的接近。</li>
</ol>
<p>RBM 使用到的数据 <code>不需要有标签</code> ，所以它可以处理照片、视频、音频、传感器数据等。这些数据基本都是没有标签的，不需要我们手动为其加上标签。RBM会自动识别这别这些，通过良好的调整 weight 和 bais ，RBM 可以从数据中抽取重要特征，重构输入数据。</p>
<p><a href="https://zhuanlan.zhihu.com/p/24989699" target="_blank" rel="external">RBM</a><br><a href="https://www.zhihu.com/question/41490383" target="_blank" rel="external">AE</a></p>
<h2 id="深度信念网络-DBN"><a href="#深度信念网络-DBN" class="headerlink" title="深度信念网络 DBN"></a>深度信念网络 DBN</h2><p>RMB 是如何解决梯度消失问题的呢？把一些 RBM 组合起来，然后使用一个巧妙的训练方法，于是我们获得了一个新的模型，并解决了梯度消失。下面我们就来看一下这个新的模型——深度信念网络。</p>
<p>如同 RBM 一样，DBN 的提出也是为了替代 反向传播。<br>从网络的结构来说，DBN 和 MLP（多层感知机） 是类似的，但从训练的角度来看，他们却是完全不同的。<br>事实上，训练方法的差异是 DBN 能够超越其他浅层网络的关键。</p>
<p>一个 DBN 可以看做是许多 RBM 组合在一起，前面 RBM 的隐藏层将作为后面 RBM 的可见层。<br>过程：</p>
<ol>
<li>第一个 RBM 被尽可能精确的训练，使其能重构它的输出</li>
<li>第一个 RBM 的隐藏层会被当做第二个 RBM 的可见层，第二个 RBM 使用第一个 RBM 的输出来训练。</li>
<li>这个过程不断重复直到网络中每一层都被训练好了。</li>
</ol>
<p><strong>也就是说， DBN 的每一层都学习到了最原始的输入。</strong><br>在其他网络结构中，前面的网络层检测简单的模式，而后面的网络则把他们结合起来。比如 CNN ，在面部识别的应用中，前面的层会检测图片中的边，后面的层使用前面层的结果，来形成面部五官特征，后面的层再使用前面层的结果，是别人脸。<br><strong>另一方面，DBN 则会根据完整的输入，慢慢更新整个模型，这就类似于相机镜头，慢慢聚焦一张图片。</strong></p>
<p>在初始训练之后，RBM 模型能够检测数据的内在模式，但我们仍然不知道如何准确分类这些数据。为了完成训练，我训需要通过监督学习的方式引入标签，优化模型。我们需要一个<strong>非常小的带标签的样本</strong>，这样才能将模式关联到相应的标签名。 weight 和 bais 的不断调整，会使整个网络的模式感知慢慢调整，经常能够使最终的准确性有一个小幅提升。</p>
<p>DBN 的优势：</p>
<ol>
<li>DBN 只需要非常少的有标签数据，这对实际应用非常有用。</li>
<li>比浅层网络训练效果好。</li>
<li>解决了梯度消失问题。</li>
</ol>
<h1 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 CNN"></a>卷积神经网络 CNN</h1><p>CNN 是在计算机视觉中占据主导作用的深度网络。</p>
<p>CNN 由很多层组成。</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>对于一张 digit image ，我们想要发现其中的特定模式，如颜色对比度或者线条等。我们可以想象使用一个手电筒去寻找一种模式，将图片分为 m<em>n 个相等大小的网格，手电的光圈（相当于<code>过滤器</code>）照在图片上，刚好覆盖一个网格，光圈不断在网格间移动，来判断某个模式是否存在，以及存在的区域。<br><strong>即，一个神经元</strong>（一个手电）<strong>仅连接固定数目的神经元</strong>（照到的区域大小是固定的，不同区域里面所包含的神经元数也是固定的）<strong>，连接的 weight 和 bais 是固定的</strong>（手电A在扫描所有网格时，都是通过 W_A </em> x_i + b_A 来识别是否有他想要的模式；手电B在扫描所有网格时，都是通过 W_B <em> x_i + b_B 来识别是否有他想要的模式）<em>*，这使得过滤器能够在图片的不同部分寻找相同的模式；而这种网格的划分，能确保我们能完全扫描整张图片。</em></em><br>当我们有许多个手电筒时，每个手电筒代表 CNN 中的一个神经元，负责寻找图片中的不同模式。每个卷积层就是由许多个这样的‘手电筒’构成的。</p>
<p>再重复一下扫描一张图片的过程，我们先将图片划分成许多小网格，然后拿卷积层的许多个手电一行一行的依次照射这些小网格。首先，全部的手电会先照射到[1, 1]这个格，每个手电筒会分别识别一下这个网格中是否有他想要识别的模式，即 W*x+b 的计算；然后全部的网格一起移到[1, 2]，判断这里有没有他们想识别的模式；……；第一行完了再去扫描第二行，直到完全扫描完这张图。</p>
<p>网络使用卷积操作在搜索特定模式</p>
<h2 id="RELU-线性修正单元"><a href="#RELU-线性修正单元" class="headerlink" title="RELU 线性修正单元"></a>RELU 线性修正单元</h2><p>由于 CNN 使用反向传播，于是又有可能出现梯度消失问题的。<br>根据 RELU 的数学定义，每一层的梯度差不多会是一个常量。<br>RELU 激活函数使得网络能够很好的训练，在前面几层不会出现有害的梯度减小。</p>
<h2 id="Pooling-池化"><a href="#Pooling-池化" class="headerlink" title="Pooling 池化"></a>Pooling 池化</h2><p>池化层是用于降维。<br>为了构建更复杂的模式，CNN 会把整个 卷积层 和 RELU层 堆起来。这样就可能导致，侦测出来的模式的数量会变得非常大。<br>池化层 能够让网络关注到 卷积层 和 RELU层 发现的最重要且相关的模式，降低 CNN 对内存的需求。</p>
<h2 id="全关联层"><a href="#全关联层" class="headerlink" title="全关联层"></a>全关联层</h2><p>以上三种组件堆起来的网络可以发现数据的模式，但依然不能知道每种模式代表什么，所以最后需要加一个 全关联层 ，来帮助网络对数据样本进行分类。</p>
<p>一个完整的神经网络由 <code>卷基层</code> 、 <code>RELU层</code> 、 <code>Pooling层</code> 这三种组件组成，每个组件都会重复出现多次，最后使用一个 <code>全关联层</code> ，完成网络对数据的分类。</p>
<p><strong>CNN 的缺点：</strong><br>属于监督学习的一种，需要大量的标签数据用来训练，在现实运用中，这些数据是很难获取的。</p>
<hr>
<p><a href="https://www.nowcoder.com/courses/190" target="_blank" rel="external">参考视频链接</a></p>
</div><div class="tags"><a href="/tags/CNN/">CNN</a><a href="/tags/RNN/">RNN</a><a href="/tags/RBM/">RBM</a><a href="/tags/DBN/">DBN</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/08/05/test/" class="next">机器学习1：有趣的机器学习</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#不同的深度神经网络适合的任务"><span class="toc-text">不同的深度神经网络适合的任务</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#深度神经网络遇到的困难"><span class="toc-text">深度神经网络遇到的困难</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#如何克服梯度消失问题？"><span class="toc-text">如何克服梯度消失问题？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#受限玻尔兹曼机-RBM"><span class="toc-text">受限玻尔兹曼机 RBM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度信念网络-DBN"><span class="toc-text">深度信念网络 DBN</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#卷积神经网络-CNN"><span class="toc-text">卷积神经网络 CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#卷积层"><span class="toc-text">卷积层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RELU-线性修正单元"><span class="toc-text">RELU 线性修正单元</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pooling-池化"><span class="toc-text">Pooling 池化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#全关联层"><span class="toc-text">全关联层</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/11/Deep-Learning/">【深度学习】深度神经网络入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/test/">机器学习1：有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/10/Tensorflow5/">【神经网络】通过代码学习Tensorflow5</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/05/Tensorflow4/">【神经网络】通过代码学习Tensorflow4</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/04/Tensorflow3/">【神经网络】通过代码学习Tensorflow3</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/dropout/" style="font-size: 15px;">dropout</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/RBM/" style="font-size: 15px;">RBM</a> <a href="/tags/DBN/" style="font-size: 15px;">DBN</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>