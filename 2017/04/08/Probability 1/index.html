<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>概率论基础（1） | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">概率论基础（1）</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">概率论基础（1）</h1><div class="post-meta"><a href="/2017/04/08/Probability 1/#comments" class="comment-count"></a><p><span class="date">Apr 08, 2017</span><span><a href="/categories/深度学习笔记/" class="category">深度学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p><strong>内容</strong>：2017年4月8日 星期六 PRML 概率论（1）。<br><strong>说明</strong>：<strong><em>PRML</em></strong>第一章，关于概率论简单介绍。<br><strong>状态</strong>：部分公式待完善</p>
<a id="more"></a>
<p><strong>学习资料</strong></p>
<ul>
<li>PRML 第一章第二章</li>
<li>概率论与信息学基础知识。英文的看起来太费劲，先看中文的。</li>
</ul>
<hr>
<p>PRML 第一章</p>
<hr>
<h1 id="概率论"><a href="#概率论" class="headerlink" title="概率论"></a>概率论</h1><p>在模式识别领域的⼀个关键概念是<strong>不确定性</strong>的概念。它可以由测量的误差引起，也可以由数据集的有限⼤⼩引起。概率论提供了⼀个合理的框架，⽤来对不确定性进⾏量化和计算。概率论还构成了模式识别的⼀个中⼼基础。当与决策论结合，概率论让我们能够根据所有能得到的信息做出最优的预测，即使信息可能是不完全的或者是含糊的。</p>
<blockquote>
<p>假设我们由两个盒⼦，⼀个红⾊的，⼀个蓝⾊的，红盒⼦中有2个苹果和6个橘⼦，蓝盒⼦中有3个苹果和1个橘⼦（如图1.9所⽰）。现在假定我们随机选择⼀个盒⼦，从这个盒⼦中我们随机选择⼀个⽔果，观察⼀下选择了哪种⽔ 果，然后放回盒⼦中。假设我们重复这个过程很多次。假设我们在40%的时间中选择红盒⼦，在60%的时间中选择蓝盒⼦，并且我们选择盒⼦中的⽔果时是等可能选择的。<br>在这个例⼦中，我们要选择的盒⼦的颜⾊是⼀个随机变量，记作B。类似地，⽔果的种类也是⼀个随机变量，记作F。</p>
</blockquote>
<p>我们把⼀个事件的概率定义为事件发⽣的次数与试验总数的⽐值，假设总试验次数趋于⽆穷。</p>
<p>注意，根据定义，概率⼀定位于区间[0,1]内。并且，如果事件是相互独⽴的，那么我们看到那些事件的概率的和⼀定等于1。</p>
<p>概率论的两个基本规则：</p>
<p><strong><em>加和规则（sum rule）</em></strong></p>
<p>$$p(X) = \sum_Yp(X,Y)$$</p>
<p><strong><em>乘积规则（product rule）</em></strong></p>
<p>$$p(X,Y) = p(Y|X)p(X)$$</p>
<p>这⾥p(X,Y)是联合概率，可以表述为“X且Y的概率”。类似地，p(Y|X)是条件概率，可以表述为“给定X的条件下Y的概率”，p(X)是边缘概率，可以简单地表述为“X的概率”。</p>
<p><strong><em>贝叶斯定理（Bayes’ theorem）</em></strong></p>
<p>$$p(Y|X)=\frac{p(X|Y)p(Y)}{p(X)}$$</p>
<p>我们可以把贝叶斯定理的分母看做<em>归⼀化常数</em>，⽤来确保公式左侧的条件概率对于所有的Y的取值之和为1。</p>
<p>使⽤加和规则，贝叶斯定理中的分母可以⽤出现在分⼦中的项表⽰：</p>
<p>$$p(X) = \sum_Yp(X|Y)p(Y)$$</p>
<p>我们可以按照下⾯的⽅式表述贝叶斯定理。如果在我们知道⽔果的种类之前，有⼈问我们哪个盒⼦被选中， 那么我们能够得到的最多的信息就是概率p(B)。 我们把这个叫做<strong><em>先验概率（prior probability）</em></strong>，因为它是在我们观察到⽔果种类之前就能够得到的概率。⼀旦我们知道⽔果是橘⼦， 我们就能够使⽤贝叶斯定理来计算概率p(B|F)。这个被称为<strong><em>后验概率（posterior probability）</em></strong>，因为它是我们观察到F之后的概率。注意，在这个例⼦中，选择红盒⼦的先验概率是 $\frac4{10}$，所以与红盒⼦相⽐，我们更有可能选择蓝盒⼦。然⽽，⼀旦我们观察到选择的⽔果是橘⼦，我们发现红盒⼦的后验概率现在是$\frac23$，因此现在实际上更可能选择的是红盒⼦。这个结果与我们的直觉相符，因为红盒⼦中橘⼦的⽐例⽐蓝盒⼦⾼得多，因此观察到⽔果是橘⼦这件事提供给我们更强的证据来选择红盒⼦。事实上，这个证据相当强，已经超过了先验的假设，使得红盒⼦被选择的可能性⼤于蓝盒⼦。</p>
<p>最后，如果两个变量的联合分布可以分解成两个边缘分布的乘积，即p(X,Y) = p(X)p(Y)，那么我们说X 和Y相互独⽴（independent）。根据乘积规则，我们可以得到p(Y|X) = p(Y)， 因此对于给定X的条件下的Y的条件分布实际上独⽴于X的值。例如，在我们的⽔果盒⼦的例⼦中，如果每个盒⼦包含同样⽐例的苹果和橘⼦，那么p(F|B) = P(F)，从⽽选择苹果的概率就与选择了哪个盒⼦⽆关。</p>
<h2 id="概率密度"><a href="#概率密度" class="headerlink" title="概率密度"></a>概率密度</h2><p>概率密度（probability density）<br>累积分布函数（cumulative distribution function）<br>概率质量函数（probability mass function）<br>    注意，如果x是⼀个离散变量，那么p(x)有时被叫做概率质量函数（probability mass function），因为它可以被看做集中在合法的x值处的“概率质量”的集合。<br>概率质量函数（probability mass function）</p>
<h2 id="期望和协⽅差"><a href="#期望和协⽅差" class="headerlink" title="期望和协⽅差"></a>期望和协⽅差</h2><p>期望<br>方差<br>协方差</p>
<h2 id="贝叶斯概率"><a href="#贝叶斯概率" class="headerlink" title="贝叶斯概率"></a>贝叶斯概率</h2><blockquote>
<p>本章⽬前为⽌，我们根据随机重复事件的频率来考察概率。我们把这个叫做经典的（classical）或者频率学家（frequentist）的关于概率的观点。现在我们转向更加通⽤的贝叶斯（Bayesian）观点。这种观点中，频率提供了不确定性的⼀个定量化描述。</p>
</blockquote>
<p>正如我们将看到的，在我们对数量（例如多项式曲线拟合例⼦中的参数$\omega$）进⾏推断时，我们可以采⽤⼀个类似的⽅法。在观察到数据之前，我们有⼀些关于参数$\omega$的假设，这以先验概率$p(\omega)$的形式给出。观测数据$D=\lbrace t_1,t_2,\ldots,t_n\rbrace$的效果可以通过条件概率$p(D|\omega)$表达。贝叶斯定理的形式为</p>
<p>$$p(\omega|D)=\frac{p(D|\omega)p(\omega)}{p(D)}$$</p>
<p>贝叶斯定理右侧的量$p(D|\omega)$由观测数据集D来估计，可以被看成参数向量$\omega$的函数，被称为<strong>似然函数（likelihood function）</strong>。它表达了在不同的参数向量$\omega$下，观测数据出现的可能性的⼤⼩。注意，似然函数不是$\omega$的概率分布，并且它关于w的积分并不（⼀定）等于1。</p>
<p>给定似然函数的定义，我们可以⽤⾃然语⾔表述贝叶斯定理</p>
<p>$$posterior \propto likelihood \times prior$$</p>
<p>上式的分母是⼀个归⼀化常数，确保了左侧的后验概率分布是⼀个合理的概率密度，积分为1。实际上，对上式的两侧关于$\omega$进⾏积分， 我们可以⽤后验概率分布和似然函数来表达贝叶斯定理的分母</p>
<p>$$p(D)=\int p(D|\omega)p(\omega)d\omega$$</p>
<p>在贝叶斯观点和频率学家观点中，似然函数$p(D|\omega)$都起着重要的作⽤。然⽽，在两种观点中，使⽤的⽅式有着本质的不同。在频率学家的观点中，$\omega$被认为是⼀个固定的参数，它的值由某种形式的“估计”来确定，这个估计的误差通过考察可能的数据集D的概率分布来得到。相反，从贝叶斯的观点来看，只有⼀个数据集D（即实际观测到的数据集），参数的不确定性通过$\omega$的概率分布来表达。</p>
<h2 id="⾼斯分布"><a href="#⾼斯分布" class="headerlink" title="⾼斯分布"></a>⾼斯分布</h2><p>正态分布（normal distribution）或者⾼斯分布（Gaussian distribution）</p>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-123621.jpg" alt=""><br><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-123842.jpg" alt=""></p>
<p>D维向量x的⾼斯分布<br>⾼斯分布的似然函数</p>
<p><strong>使⽤⼀个观测数据集来决定概率分布的参数</strong>的⼀个通⽤的标准是<strong><em>寻找使似然函数取得最⼤值的参数值</em></strong>。这个标准看起来可能很奇怪，因为从我们之前对于概率论的讨论来看，似乎在给定数据集的情况下最⼤化概率的参数（⽽不是在给定参数的情况下最⼤化数据集出现的概率）是更加⾃然的。事实上，这两个标准是相关的。我们后⾯将使⽤曲线拟合的例⼦来说明这⼀点。</p>
<p>对数似然函数<br>偏移（bias）<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-124731.jpg" alt=""></p>
<p>在⼀个纯粹的贝叶斯⽅法中， 我们应该⾃始⾄终地应⽤概率的加和规则和乘积规则。</p>
<h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><p>在最⼤似然⽅法中，由于<strong>过拟合</strong>现象，模型在训练集上的表现并不能很好地表⽰模型对于未知数据的预测能⼒。<br>交叉验证（cross validation）<br> “留⼀法”（leave-one-out）<br> 缺点：耗时；有多个复杂度参数，探索这些参数的组合所需的训练次数可能是参数个数的指数函数。<br>理想情况下，<strong>模型的选择应该只依赖于训练数据</strong>，并且应该允许在⼀轮训练中对⽐多个超参数以及模型类型。<br>信息准则<br> ⾚池信息准则（Akaike information criterion）<br> 贝叶斯信息准则（Bayesian information criterion）</p>
<h2 id="纬度灾难"><a href="#纬度灾难" class="headerlink" title="纬度灾难"></a>纬度灾难</h2><p>把输⼊空间划分成⼩的单元格<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-130330.jpg" alt=""><br>如果我们把空间的区域分割成⼀个个的单元格，那么这些<strong>单元格的数量会随着空间的维数以指数的形式增⼤</strong>。当单元格的数量指数增⼤时，为了保证单元格不为空，我们就不得不需要指数量级的训练数据。很明显，我们只能在变量数量相当少的情况下才能使⽤这种⽅法，因此我们需要寻找⼀些更⾼级的⽅法。</p>
<p>虽然维度灾难在模式识别应⽤中是⼀个重要的问题，但是它并不能阻⽌我们寻找应⽤于⾼维空间的有效技术。原因有两⽅⾯。<strong>第⼀</strong>，真实的数据经常被限制在有着较低的有效维度的空间区域中，特别地，在⽬标值会发⽣重要变化的⽅向上也会有这种限制。<strong>第⼆</strong>，真实数据通常⽐较光滑（⾄少局部上⽐较光滑），因此⼤多数情况下，对于输⼊变量的微⼩改变，⽬标值的改变也很⼩，因此对于新的输⼊变量，我们可以通过局部的类似于插值的技术来进⾏预测。</p>
<hr>
<h1 id="决策论"><a href="#决策论" class="headerlink" title="决策论"></a>决策论</h1><p>假设我们有⼀个输⼊向量x和对应的⽬标值向量t， 我们的⽬标是对于⼀个新的x值，预测t。对于回归问题，t由连续变量组成，⽽对于分类问题，t表⽰类别标签。 联合概率 分布p(x,t)完整地总结了与这些变量相关的不确定性。从训练数据集中确定p(x,t)是推断（inference）问题的⼀个例⼦，并且通常是⼀个⾮常难的问题。对这种问题的解答是本书⼤部分内容的主题。但是在⼀个实际应⽤中，我们经常必须对t的值做出具体的预测，或者更⼀般地，根据我们对于t的可能取值的理解，采取⼀个具体的动作。这⼀⽅⾯就是决策论的主题。</p>
<p>在给定合适的概率的前提下，如何进⾏最优的决策。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-132619.jpg" alt=""></p>
<h2 id="最⼩化错误分类率"><a href="#最⼩化错误分类率" class="headerlink" title="最⼩化错误分类率"></a>最⼩化错误分类率</h2><p>尽可能少地作出错误分类</p>
<p>决策区域（decision region）<br>决策边界（decision boundary）或者决策⾯（decision surface）</p>
<p>最⼩化p(mistake) 最⼤化正确率<br>$$p(mistake) = p(x\in R_1,C_2)+p(x\in R_2,C_1)$$</p>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-134358.jpg" alt=""><br><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-134325.jpg" alt=""></p>
<h2 id="最⼩化期望损失"><a href="#最⼩化期望损失" class="headerlink" title="最⼩化期望损失"></a>最⼩化期望损失</h2><p>损失函数（loss function） 代价函数（cost function）<br>效⽤函数（utility function） 最⼤化</p>
<h2 id="拒绝选项"><a href="#拒绝选项" class="headerlink" title="拒绝选项"></a>拒绝选项</h2><p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-134650.jpg" alt=""></p>
<h2 id="推断和决策"><a href="#推断和决策" class="headerlink" title="推断和决策"></a>推断和决策</h2><p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-134850.jpg" alt=""></p>
<h2 id="回归问题的损失函数"><a href="#回归问题的损失函数" class="headerlink" title="回归问题的损失函数"></a>回归问题的损失函数</h2><hr>
<h1 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h1><p>我们考虑⼀个离散的随机变量x。当我们观察到这个变量的⼀个具体值的时候，我们接收到了多少信息呢？信息量可以被看成在学习x的值的时候的“惊讶程度”。如果有⼈告诉我们 ⼀个相当不可能的时间发⽣了， 我们收到的信息要多于我们被告知某个很可能发⽣的事件发⽣时收到的信息。 如果我们知道某件事情⼀定会发⽣， 那么我们就不会接收到信息。</p>
<p>于是，我们对于信息内容的度量将依赖于概率分布p(x)，因此我们想要寻找⼀个函数h(x)，它是概率p(x)的单调递增函数，表达了信息的内容。h(·)的形式可以这样寻找：<strong>如果我们有两个不相关的事件x和y ， 那么我们观察到两个事件同时发⽣时获得的信息应该等于观察到事件各⾃发⽣时获得的信息之和，即h(x,y) = h(x)+h(y)</strong>。两个不相关事件是统计独⽴的，因此p(x,y) = p(x)p(y)。根据这两个关系，很容易看出h(x)⼀定与p(x)的对数有关。因此，我们有</p>
<p>$$h(x) = − \log_2 p(x)$$</p>
<p>h(x)的单位是⽐特（bit, binary digit）。</p>
<p>现在假设⼀个发送者想传输⼀个随机变量的值给接收者。这个过程中，他们传输的平均信息量通的期望值为</p>
<p>$$ H[x] = - \sum_x p(x)\log_2 p(x)$$</p>
<p>这个重要的量被叫做随机变量x的熵（entropy）。注意，$\lim_{p\to 0} p\log_2 p=0$，因此只要我们遇到⼀个x使得p(x) = 0，那么我们就应该令$p(x)\log_2 p(x)=0$。</p>
<p>⽆噪声编码定理（noiseless coding theorem）表明，熵是传输⼀个随机变量状态值所需的⽐特位的下界。</p>
<p>现在开始，我们会把熵的定义中的对数变成⾃然对数，因为这样做会使得熵的概念与本书后续章节中的思想结合起来⽐较⽅便。这种情况下，熵的度量的单位是nat，⽽不是bit。两者的差别是⼀个ln2的因⼦。</p>
<p>随机变量X的熵:</p>
<p>$$ H[x] = - \sum_i p(x_i)\ln p(x_i)$$</p>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-141636.jpg" alt=""></p>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-04-08-141711.jpg" alt=""></p>
<h2 id="相对熵和互信息"><a href="#相对熵和互信息" class="headerlink" title="相对熵和互信息"></a>相对熵和互信息</h2><p>Kullback-Leibler散度（Kullback-Leibler divergence），或者KL散度</p>
</div><div class="tags"><a href="/tags/数学/">数学</a><a href="/tags/概率论/">概率论</a><a href="/tags/信息学/">信息学</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/04/09/Probability-2/" class="pre">概率论基础（2）</a><a href="/2017/04/08/Markdown-Picture/" class="next">Markdown中的图片处理</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#概率论"><span class="toc-text">概率论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#概率密度"><span class="toc-text">概率密度</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#期望和协⽅差"><span class="toc-text">期望和协⽅差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#贝叶斯概率"><span class="toc-text">贝叶斯概率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#⾼斯分布"><span class="toc-text">⾼斯分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#模型选择"><span class="toc-text">模型选择</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#纬度灾难"><span class="toc-text">纬度灾难</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#决策论"><span class="toc-text">决策论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#最⼩化错误分类率"><span class="toc-text">最⼩化错误分类率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#最⼩化期望损失"><span class="toc-text">最⼩化期望损失</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#拒绝选项"><span class="toc-text">拒绝选项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#推断和决策"><span class="toc-text">推断和决策</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#回归问题的损失函数"><span class="toc-text">回归问题的损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#信息论"><span class="toc-text">信息论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#相对熵和互信息"><span class="toc-text">相对熵和互信息</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/07/Tensorflow/">【神经网络】通过代码学习Tensorflow</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/test/">机器学习1：有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/06/ScikitLearn/">【机器学习】通过代码学习scikit-learn</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/23/指针初步/">【C++】指针初步</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/22/二维数组/">【C++】二维数组的使用</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>