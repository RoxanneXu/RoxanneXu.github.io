<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>《神经网络和机器学习》阅读笔记2 | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">《神经网络和机器学习》阅读笔记2</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">《神经网络和机器学习》阅读笔记2</h1><div class="post-meta"><a href="/2017/06/12/NN-and-DL-2/#comments" class="comment-count"></a><p><span class="date">Jun 12, 2017</span><span><a href="/categories/深度学习笔记/" class="category">深度学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p><strong>时间</strong>：2017年6月12日 星期一<br><strong>内容</strong>：《神经网络和机器学习》阅读笔记2 反向传播算法是如何工作的<br><strong>参考</strong>：<a href="https://mp.weixin.qq.com/s/t-UPSoVnwSD7XT6IxbE-Wg" target="_blank" rel="external">神经网络和机器学习</a><br><a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="external">Neural Networks and Deep Learning</a></p>
<a id="more"></a>
<h1 id="一个基于矩阵的快速计算神经网络输出的方法"><a href="#一个基于矩阵的快速计算神经网络输出的方法" class="headerlink" title="一个基于矩阵的快速计算神经网络输出的方法"></a>一个基于矩阵的快速计算神经网络输出的方法</h1><p>我们先介绍一种符号来表示网络中的权重参数，这种表示法不会引发歧义。我们用$w^l_{jk}$来表示从第<code>l−1</code>层的第<code>k</code>个神经元到第<code>l</code>层的第<code>j</code>个神经元的连接的权重。例如，下图展示了从第二层的第四个神经元到第三层的第二个神经元的连接的权重：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-085315.jpg" alt=""><br>我们用一种相似的记法来表示网络的偏置和激活值。确切地，我们用$b^l_j$表示第<code>l</code>层第<code>j</code>个神经元的偏置，用$a^l_j$表示第<code>l</code>层的第<code>j</code>个神经元的激活值（输出）。下图展示了这种记法的一个例子：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-085642.jpg" alt=""><br>所以有：<img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-090157.jpg" alt=""><br>改写成矩阵形式：$a^l=\sigma(w^la^{l-1}+b^l)$.</p>
<hr>
<h1 id="关于损失函数的两个假设"><a href="#关于损失函数的两个假设" class="headerlink" title="关于损失函数的两个假设"></a>关于损失函数的两个假设</h1><p>损失函数具有以下形式：<br>$$C=\frac 1{2n}\sum_x\Vert y(x)-a^L(x)\Vert^2$$<br>其中n是训练样本总数；求和符号表示对每个独立训练样本x求和；y=y(x)是对应的希望输出；L是神经网络层数；$a^L=a^L(x)$是输入为x时激活函数的输出向量。<br><strong>假设一</strong><br>代价函数能够被写成$C=\frac 1n\sum_xC_x$的形式，其中$C_x$是每个独立训练样本x的代价函数。在代价函数为平方代价函数的情况下，一个训练样本的代价是。<br><strong>假设二</strong><br>C可以写成关于神经网络输出结果的函数：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-101448.jpg" alt=""><br>损失函数满足该要求，因为单一训练样本x的二次代价可以表示为：<br>$$C=\frac 12\Vert y-a^L\Vert^2=\frac 12\sum_j(y_j-a^L_j)^2$$</p>
<hr>
<h1 id="Hadamard积"><a href="#Hadamard积" class="headerlink" title="Hadamard积"></a>Hadamard积</h1><p>假设s和t是两个有相同维数的向量。那么我们用s⊙t来表示两个向量的对应元素(elementwise)相乘。因此s⊙t的元素 $(s⊙t)_j = s_jt_j$。</p>
<hr>
<h1 id="反向传播背后的四个基本等式"><a href="#反向传播背后的四个基本等式" class="headerlink" title="反向传播背后的四个基本等式"></a>反向传播背后的四个基本等式</h1><p><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-13-071944.jpg" alt=""></p>
<p><a href="https://mp.weixin.qq.com/s?__biz=MzIxMjAzNDY5Mg==&amp;mid=400329443&amp;idx=1&amp;sn=f7158ee615c2a0d6f0014adae038193e&amp;scene=21#wechat_redirect" target="_blank" rel="external">详细内容戳这里</a></p>
<hr>
<h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>反向传播等式为我们提供了一个计算代价函数梯度的方法。下面让我们明确地写出该算法：</p>
<ol>
<li><p>输入x:计算输入层相应的激活函数值$a^1$。</p>
</li>
<li><p>正向传播：对每个$l=2,3,…,L$，计算$z^l=w^la^{l-1}+b^l$和$a^l=σ(z^l)$。</p>
</li>
<li><p>输出误差$δ^L$：计算向量$δ^L=∇_aC⊙σ′(z^L)$。</p>
</li>
<li><p>将误差反向传播：对每个$l=L−1,L−2,…,2$计算$δ^l=((w^{l+1})^T δ^{l+1})⊙σ′(z^l)$。</p>
</li>
<li><p>输出：代价函数的梯度为$\frac{\partial C}{\partial w^l_{jk}}=a^{l-1}_kδ^l_j$和$\frac{\partial C}{\partial b^l_j}=δ^l_j$。</p>
</li>
</ol>
<p>通过上述过程就能看出它为什么叫<strong>反向</strong>传播算法。我们从最后一层开始，反向计算错误向量$^l$。在神经网络中反向计算误差可能看起来比较奇怪。但如果回忆反向传播的证明过程，会发现反向传播的过程起因于代价函数是关于神经网络输出值的函数。为了了解代价函数是如何随着前面的权重和偏移改变的，我们必须不断重复应用链式法则，通过反向的计算得到有用的表达式。</p>
<h1 id="反向传播的优点"><a href="#反向传播的优点" class="headerlink" title="反向传播的优点"></a>反向传播的优点</h1><p>反向传播的优点在于它仅利用一次前向传播就可以同时计算出所有的偏导$\frac{\partial C}{\partial w_j}$，随后也仅需要一次反向传播。大致来说，反向传播算法所需要的总计算量与两次前向传播的计算量基本相等（这应当是合理的，但若要下定论的话则需要更加细致的分析。合理的原因在于前向传播时主要的计算量在于权重矩阵的乘法计算，而反向传播时主要的计算量在于权重矩阵转置的乘法。很明显，它们的计算量差不多）。这与一百万零一次前向传播相比，虽然反向传播看起来更复杂一些，但它确实更更更更更快。</p>
<h1 id="反向传播的整体描述"><a href="#反向传播的整体描述" class="headerlink" title="反向传播的整体描述"></a>反向传播的整体描述</h1></div><div class="tags"></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/06/13/剑指offer-1-10/" class="pre">剑指offer 1~10</a><a href="/2017/06/08/NN-and-DL/" class="next">《神经网络和机器学习》阅读笔记1</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#一个基于矩阵的快速计算神经网络输出的方法"><span class="toc-text">一个基于矩阵的快速计算神经网络输出的方法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#关于损失函数的两个假设"><span class="toc-text">关于损失函数的两个假设</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Hadamard积"><span class="toc-text">Hadamard积</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播背后的四个基本等式"><span class="toc-text">反向传播背后的四个基本等式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播算法"><span class="toc-text">反向传播算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播的优点"><span class="toc-text">反向传播的优点</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#反向传播的整体描述"><span class="toc-text">反向传播的整体描述</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/09/Tensorflow5/">【神经网络】通过代码学习Tensorflow5</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/test/">机器学习1：有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/05/Tensorflow4/">【神经网络】通过代码学习Tensorflow4</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/04/Tensorflow3/">【神经网络】通过代码学习Tensorflow3</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/02/Tensorflow2/">【神经网络】通过代码学习Tensorflow2</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/dropout/" style="font-size: 15px;">dropout</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>