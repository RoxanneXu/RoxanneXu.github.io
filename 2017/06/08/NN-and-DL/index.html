<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>《神经网络和机器学习》阅读笔记1 | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">《神经网络和机器学习》阅读笔记1</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">《神经网络和机器学习》阅读笔记1</h1><div class="post-meta"><a href="/2017/06/08/NN-and-DL/#comments" class="comment-count"></a><p><span class="date">Jun 08, 2017</span><span><a href="/categories/深度学习笔记/" class="category">深度学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p><strong>时间</strong>：2017年6月8日 星期四<br><strong>内容</strong>：《神经网络和机器学习》阅读笔记1 使用神经网络识别手写数字<br><strong>参考</strong>：<a href="https://mp.weixin.qq.com/s/t-UPSoVnwSD7XT6IxbE-Wg" target="_blank" rel="external">神经网络和机器学习</a><br><a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="external">Neural Networks and Deep Learning</a></p>
<a id="more"></a>
<p><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-08-073912.jpg" alt=""></p>
<hr>
<h1 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h1><p><strong>输入</strong>x1,x2,x3,… 值取0或1<br><strong>输入</strong>y 值取0或1<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-051751.jpg" alt=""><br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-051854.jpg" alt=""><br><strong>权重（weight）</strong> w1,w2,… 实数，来表示各个输入对于输出的重要程度。<br><strong>偏差（bias）</strong> b 感知机为了得到输出为1的容易度的度量。如果从生物的角度来理解，偏移是使神经元被激活的容易度的度量。</p>
<hr>
<h1 id="sigmoid神经元"><a href="#sigmoid神经元" class="headerlink" title="sigmoid神经元"></a>sigmoid神经元</h1><p><strong>输入</strong>x1,x2,x3,… 值取0到1之间的任意浮点数<br><strong>输入</strong>$\sigma (wx+b)$ 值取0到1之间的任意浮点数，其中$\sigma$被称为sigmoid函数，由下图可以看出使用sigmiod函数我们就得到了一个平滑的感知机。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-051411.jpg" alt=""><br>$$\sigma(z)=\frac1{1+e^{-z}}=\frac1{1+e^{-\sum_j w_jx_j-b}}$$<br><strong>sigmoid函数</strong>σ函数的平滑属性才是其关键，不用太在意它的具体代数形式。σ函数的平滑属性意味着当我们在权值和偏差上做出值为Δwj，Δb的轻微改变时，神经元的输出也将只是轻微地变化Δoutput。事实上，由微积分的知识可知，Δoutput近似于：$\Delta output\approx\sum_j\frac{\partial output}{\partial w_j}\Delta w_j+\frac{\partial output}{\partial b}\Delta b$<br><strong><em>Δoutput是关于权值和偏差的改变量Δwj和Δb的线性函数（linear function）。这种线性属性，使得选择权值和偏差的轻微改变量并使输出按照预期发生小幅度变化成为易事。</em></strong><br> 如果真的只是σ的形状起作用而其具体代数形式没有什么用的话，为什么公式(3)要把σ表示为这种特定的形式？事实上，在书的后面部分我们也会偶尔提到一些在输出f(w⋅x+b)中使用其它激活函数（activation function）f(⋅)的神经元。当我们使用其它不同的激活函数时主要改变的是公式(5)中偏微分的具体值。在我们需要计算这些偏微分值之前，使用σ将会简化代数形式，因为指数函数在求微分时有着良好的性质。不管怎样，σ在神经网络工作中是最常被用到的，也是本书中最频繁的激活函数。<br><strong>权重（weight）</strong> w1,w2,… 实数，来表示各个输入对于输出的重要程度。<br><strong>偏差（bias）</strong> 感知机为了得到输出为1的容易度的度量。如果从生物的角度来理解，偏差是使神经元被激活的容易度的度量。</p>
<p><strong>感知机与sigmoid神经元的对比</strong><br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-052347.jpg" alt=""><br>我们希望通让网络学习如何去解决问题。对于一个以手写数字的扫描图像的原始像素数据作为输入的网络，我们想要学习权重和偏差以便最终能正确的分类这些数字。<br>在后面的学习中我们会发现，只有当<strong>我们对网络的一些权值（或偏差）做出一些微小的改变后，对输出结果带来相应的改变，并且这种改变也是轻微的</strong>，才能使学习变得可能。<br>如果满足在权值（或偏差）上的小改变只会引起输出上的小幅变化这一性质，那么以此性质为基础，我们就可以改变权值和偏差来使得网络的表现越来越接近我们预期。例如，假设原始的网络会将一张写着「9」的手写数字图片错误分类为「8」。我们可以尝试找到一个正确的轻微改变权值和偏差的方法，来使得我们网络的输出更接近于正确答案——将该图片分类为「9」。重复这个过程，不断地修改权值和偏差并且产生越来越好的结果。这样我们的网络就开始学习起来了。<br>轻微改变网络中任何一个感知机的权值或偏差有时甚至会导致感知机的输出完全翻转——比如说从0变为1。 这个翻转行为可能以某种非常复杂的方式彻底改变网络中其余部分的行为。<br>sigmoid神经元与感知机有些相似，但做了一些修改使得我们在轻微改变其权值和偏差时只会引起小幅度的输出变化。这是使由sigmoid神经元构成的网络能够学习的关键因素。</p>
<hr>
<h1 id="神经网络的结构"><a href="#神经网络的结构" class="headerlink" title="神经网络的结构"></a>神经网络的结构</h1><p><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-055354.jpg" alt=""><br><strong>输入层 隐层 输出层</strong><br><strong>前馈神经网络（feedforward neural networks）</strong>即把上一层的输出作为下层输入的神经网络。这种网络是不存在环的——信息总是向前传播，从不反向回馈。如果我们要制造一个环，那么我们将会得到一个使σ函数输入依赖于其输出的网络。这很难去理解，所以我们并不允许存在这样的环路。<br><strong>递归神经网络（recurrent neural networks）</strong>该模型的关键在于，神经元在变为非激活态之前会在一段有限时间内均保持激活状态。这种激活状态可以激励其他的神经元，被激励的神经元在随后一段有限时间内也会保持激活状态。如此就会导致更多的神经元被激活，一段时间后我们将得到一个级联的神经元激活系统。在这个模型中环路并不会带来问题，因为神经元的输出只会在一段之间之后才影响到它的输入，它并非实时的。</p>
<hr>
<h1 id="用简单的网络结构解决手写数字识别"><a href="#用简单的网络结构解决手写数字识别" class="headerlink" title="用简单的网络结构解决手写数字识别"></a>用简单的网络结构解决手写数字识别</h1><p><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-060810.jpg" alt=""><br>我们可以把手写数字识别问题拆分为两个子问题。首先，我们要找到一种方法能够把一张包含若干数字的图像分割为若干小图片，其中每个小图像只包含一个数字。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-060837.jpg" alt=""><br>当图像被分割之后，接下来的任务就是如何识别每个独立的手写数字。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-060853.jpg" alt=""><br>我们将把精力集中在实现程序去解决第二个问题，即如何正确分类每个单独的手写数字。因为事实证明，只要你解决了数字分类的问题，分割问题相对来说不是那么困难。分割问题的解决方法有很多。一种方法是尝试不同的分割方式，用数字分类器对每一个切分片段打分。如果数字分类器对每一个片段的置信度都比较高，那么这个分割方式就能得到较高的分数；如果数字分类器在一或多个片段中出现问题，那么这种分割方式就会得到较低的分数。<strong>这种方法的思想是，如果分类器有问题，那么很可能是由于图像分割出错导致的。</strong>这种思想以及它的变种能够比较好地解决分割问题。<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-06-12-061037.jpg" alt=""><br>输入层：是对输入像素编码的神经元。我们的训练数据是一堆28乘28手写数字的位图，因此我们的输入层包含了784=28×28个神经元。输入的像素点是其灰度值，0.0代表白色，1.0代表黑色，中间值表示不同程度的灰度值。<br>隐层：我们为隐层设置了n个神经元，我们会实验n的不同取值。在这个例子中我们只展现了一个规模较小的隐层，它仅包含了n=15个神经元。<br>输出层：我们把输出层神经元依次标记为0到9，我们要找到哪一个神经元拥有最高的激活值。</p>
<p><strong>输出层为什么使用10个神经元（0~9）而不是4个（0000~1111）？</strong><br>如果是10个输出，我们可以假想隐层每个神经元都是在判断某些像素的灰度值是如何编码的（即某一小块是什么形状）。<br>如果我们有4个输出，那么第一个输出神经元将会尽力去判断数字的最高有效位是什么。把数字的最高有效位和数字的形状联系起来并不是一个简单的问题。很难想象出有什么恰当的历史原因，一个数字的形状要素会和一个数字的最高有效位有什么紧密联系。</p>
<hr>
<h1 id="通过梯度下降法学习参数"><a href="#通过梯度下降法学习参数" class="headerlink" title="通过梯度下降法学习参数"></a>通过梯度下降法学习参数</h1><p>定义一个代价函数<br>$$C(w,b)=\frac1{2n}\Vert y(x)-a\Vert^2$$<br>x为输入，28×28=784-维的向量<br>y(x)为正确结果<br>a为输出，10-维向量<br>w表示所有权重的集合，b表示所有偏差的集合</p>
<p><strong>我们的训练算法的目标就是通过调整函数的权重和偏差来最小化代价函数C(w,b)。我们将通过梯度下降法来达到这个目的。</strong><br>为什么要介绍<strong>平方代价（quadratic cost）</strong>呢？毕竟我们最初所感兴趣的内容不是对图像正确地分类么？为什么不增大正确输出的得分，而是去最小化一个像平方代价类似的间接评估呢？这么做是因为在神经网络中，被正确分类的图像的数量所关于权重、偏差的函数并不是一个平滑的函数。大多数情况下，对权重和偏差做出的微小变动并不会影响被正确分类的图像的数量。这会导致我们很难去刻画如何去优化权重和偏差才能得到更好的结果。一个类似平方代价的平滑代价函数能够更好地指导我们如何去改变权重和偏差来达到更好的效果。<br><strong>基于梯度的搜索</strong>：我们从某些初始解出发，迭代寻找最优参数值。每次迭代中，我们计算代价函数在当前点的梯度，然后根据梯度确定搜索方向。例如，由于负梯度方向是函数值下降最快的方向，因此梯度下降法就是沿着负梯度方向搜索最优解。</p>
<p>C可以是任意的多元实值函数，我们一二元为例。假定C(v)有两个变量v1和v2，希望得到C得全局最小值。<br>首先把我们的函数想象成一个山谷。我们想象又一个小球从山谷的斜坡滚落下来。我们的日常经验告诉我们这个球终会达到谷底。也许我们可以用这种思想来解决函数最小值的问题？我们随机地为小球选取一个起点，然后开始模拟小球滚落到谷底的运动。我们可以简单的通过计算C的导数（或者二阶导数）来模拟-这些导数将会告诉我们关于山谷“地形”的一切，这会引导我们的小球该如何下落。</p>
<p>为了更精确的描述这个问题，让我们想象一下如果我们在v1方向移动一个很小的量Δv1并在v2方向移动一个很小的量Δv2将会发生什么呢。通过计算可以告诉我们C将会产生如下改变：$\Delta\approx\frac{\partial C}{\partial v_1}\Delta v_1+\frac{\partial C}{\partial v_2}\Delta v_2$</p>
<p>我们用∇C来表示梯度向量，比如：$\nabla C=\left(\frac{\partial C}{\partial v_1},\frac{\partial C}{\partial v_2}\right)^T$。事实上你可以把梯度仅仅看做一个简单的数学记号，方便用来表示偏导的向量。<br>定义好上述符号之后，关于ΔC的表达式可写成如下形式：$\Delta C\approx\nabla C\Delta v$</p>
<p>假设我们选取$\Delta v=-\eta\nabla C$，这里的η是个很小的正数（就是我们熟知的学习速率），那么$\Delta C\approx-\eta\nabla C\nabla C=-\eta\Vert\nabla C\Vert^2$。由于$\Vert\nabla C\Vert^2$≥0，这保证了ΔC≤0，例如，如果我们以这种方式去改变v，那么C将一直会降低，不会增加。</p>
<p>所以对于小球的移动：v→v′=v−η∇C.然后我们可以迭代地去更新。如果我们反复那么做，那么C会一直降低直到我们想要需找的全局最小值。</p>
</div><div class="tags"><a href="/tags/感知机/">感知机</a><a href="/tags/sigmoid神经元/">sigmoid神经元</a><a href="/tags/神经网络/">神经网络</a><a href="/tags/梯度下降/">梯度下降</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/06/12/NN-and-DL-2/" class="pre">《神经网络和机器学习》阅读笔记2</a><a href="/2017/05/15/Approximation-Algorithm/" class="next">近似算法</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#感知机"><span class="toc-text">感知机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sigmoid神经元"><span class="toc-text">sigmoid神经元</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#神经网络的结构"><span class="toc-text">神经网络的结构</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#用简单的网络结构解决手写数字识别"><span class="toc-text">用简单的网络结构解决手写数字识别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#通过梯度下降法学习参数"><span class="toc-text">通过梯度下降法学习参数</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/test/">机器学习1：有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/06/ScikitLearn/">【机器学习】通过代码学习scikit-learn</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/23/指针初步/">【C++】指针初步</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/22/二维数组/">【C++】二维数组的使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/06/22/char字符串/">【C++】char型字符串的使用</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>