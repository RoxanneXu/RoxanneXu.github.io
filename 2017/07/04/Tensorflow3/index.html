<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="换了一个主题之后Letax全都渲染不出来了，口亨！"><title>【神经网络】通过代码学习Tensorflow3 | XxxSssSss</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【神经网络】通过代码学习Tensorflow3</h1><a id="logo" href="/.">XxxSssSss</a><p class="description">啦啦啦~~~</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">【神经网络】通过代码学习Tensorflow3</h1><div class="post-meta"><a href="/2017/07/04/Tensorflow3/#comments" class="comment-count"></a><p><span class="date">Jul 04, 2017</span><span><a href="/categories/机器学习笔记/" class="category">机器学习笔记</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>Tensorflow 高阶内容<br>TBC…<br><a id="more"></a></p>
<h1 id="Classification-分类学习"><a href="#Classification-分类学习" class="headerlink" title="Classification 分类学习"></a>Classification 分类学习</h1><p>分类和回归的区别在于输出变量的类型上。 通俗理解定量输出是回归，或者说是连续变量预测； 定性输出是分类，或者说是离散变量预测。如预测房价这是一个回归任务； 把东西分成几类, 比如猫狗猪牛，就是一个分类任务。<br><figure class="highlight python"><figcaption><span>如何使用TensorFlow解决Classification（分类）问题</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span>
<span class="line">2</span>
<span class="line">3</span>
<span class="line">4</span>
<span class="line">5</span>
<span class="line">6</span>
<span class="line">7</span>
<span class="line">8</span>
<span class="line">9</span>
<span class="line">10</span>
<span class="line">11</span>
<span class="line">12</span>
<span class="line">13</span>
<span class="line">14</span>
<span class="line">15</span>
<span class="line">16</span>
<span class="line">17</span>
<span class="line">18</span>
<span class="line">19</span>
<span class="line">20</span>
<span class="line">21</span>
<span class="line">22</span>
<span class="line">23</span>
<span class="line">24</span>
<span class="line">25</span>
<span class="line">26</span>
<span class="line">27</span>
<span class="line">28</span>
<span class="line">29</span>
<span class="line">30</span>
<span class="line">31</span>
<span class="line">32</span>
<span class="line">33</span>
<span class="line">34</span>
<span class="line">35</span>
<span class="line">36</span>
<span class="line">37</span>
<span class="line">38</span>
<span class="line">39</span>
<span class="line">40</span>
<span class="line">41</span>
<span class="line">42</span>
<span class="line">43</span>
<span class="line">44</span>
<span class="line">45</span>
<span class="line">46</span>
<span class="line">47</span>
<span class="line">48</span>
<span class="line">49</span>
<span class="line">50</span>
<span class="line">51</span>
<span class="line">52</span>
<span class="line">53</span>
<span class="line">54</span>
</pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span>
<span class="line"><span class="string">'''</span></span>
<span class="line"><span class="string">主要是</span></span>
<span class="line"><span class="string">激励函数选择  tf.nn.softmax</span></span>
<span class="line"><span class="string">损失函数选择  cross_entropy</span></span>
<span class="line"><span class="string">输出结果     accuracy</span></span>
<span class="line"><span class="string">'''</span></span>
<span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span>
<span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span>
<span class="line"><span class="comment"># number 1 to 10 data</span></span>
<span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>, one_hot=<span class="keyword">True</span>)</span>
<span class="line"></span>
<span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None,)</span>:</span></span>
<span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span>
<span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span>
<span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>,)</span>
<span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span>
<span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span>
<span class="line">        outputs = Wx_plus_b</span>
<span class="line">    <span class="keyword">else</span>:</span>
<span class="line">        outputs = activation_function(Wx_plus_b,)</span>
<span class="line">    <span class="keyword">return</span> outputs</span>
<span class="line"></span>
<span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_accuracy</span><span class="params">(v_xs, v_ys)</span>:</span></span>
<span class="line">    <span class="keyword">global</span> prediction</span>
<span class="line">    y_pre = sess.run(prediction, feed_dict=&#123;xs: v_xs&#125;)</span>
<span class="line">    correct_prediction = tf.equal(tf.argmax(y_pre,<span class="number">1</span>), tf.argmax(v_ys,<span class="number">1</span>))</span>
<span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span>
<span class="line">    result = sess.run(accuracy, feed_dict=&#123;xs: v_xs, ys: v_ys&#125;)</span>
<span class="line">    <span class="keyword">return</span> result</span>
<span class="line"></span>
<span class="line"><span class="comment"># define placeholder for inputs to network</span></span>
<span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">784</span>]) <span class="comment"># 28x28</span></span>
<span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span>
<span class="line"></span>
<span class="line"><span class="comment"># add output layer</span></span>
<span class="line">prediction = add_layer(xs, <span class="number">784</span>, <span class="number">10</span>,  activation_function=tf.nn.softmax)</span>
<span class="line"></span>
<span class="line"><span class="comment"># the error between prediction and real data</span></span>
<span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),</span>
<span class="line">                                              reduction_indices=[<span class="number">1</span>]))       <span class="comment"># loss</span></span>
<span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy) <span class="comment"># SGD</span></span>
<span class="line"></span>
<span class="line">sess = tf.Session()</span>
<span class="line"><span class="comment"># important step</span></span>
<span class="line">init = tf.global_variables_initializer()</span>
<span class="line">sess.run(init)</span>
<span class="line"></span>
<span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span>
<span class="line">    batch_xs, batch_ys = mnist.train.next_batch(<span class="number">100</span>)</span>
<span class="line">    sess.run(train_step, feed_dict=&#123;xs: batch_xs, ys: batch_ys&#125;)</span>
<span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span>
<span class="line">        print(compute_accuracy(</span>
<span class="line">            mnist.test.images, mnist.test.labels))</span>
</pre></td></tr></table></figure></p>
<p><code>mnist.train.next_batch(100)</code> <strong>每次从训练集中取出一部分做训练</strong>，这种 Gradient Descent 的方法叫做 SGD。这样训练不仅比每次都拿出全部数据训练要快，而且结果不一定比全训练要差。<br><figure class="highlight python"><figcaption><span>从输出结果可以看出前几轮有明显提升</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span>
<span class="line">2</span>
<span class="line">3</span>
<span class="line">4</span>
<span class="line">5</span>
<span class="line">6</span>
<span class="line">7</span>
<span class="line">8</span>
<span class="line">9</span>
<span class="line">10</span>
<span class="line">11</span>
<span class="line">12</span>
<span class="line">13</span>
<span class="line">14</span>
<span class="line">15</span>
<span class="line">16</span>
<span class="line">17</span>
<span class="line">18</span>
<span class="line">19</span>
<span class="line">20</span>
<span class="line">21</span>
<span class="line">22</span>
<span class="line">23</span>
<span class="line">24</span>
<span class="line">25</span>
<span class="line">26</span>
<span class="line">27</span>
<span class="line">28</span>
</pre></td><td class="code"><pre><span class="line">Successfully downloaded train-images-idx3-ubyte.gz <span class="number">9912422</span> bytes.</span>
<span class="line">Extracting MNIST_data/train-images-idx3-ubyte.gz</span>
<span class="line">Successfully downloaded train-labels-idx1-ubyte.gz <span class="number">28881</span> bytes.</span>
<span class="line">Extracting MNIST_data/train-labels-idx1-ubyte.gz</span>
<span class="line">Successfully downloaded t10k-images-idx3-ubyte.gz <span class="number">1648877</span> bytes.</span>
<span class="line">Extracting MNIST_data/t10k-images-idx3-ubyte.gz</span>
<span class="line">Successfully downloaded t10k-labels-idx1-ubyte.gz <span class="number">4542</span> bytes.</span>
<span class="line">Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</span>
<span class="line"><span class="number">0.1694</span></span>
<span class="line"><span class="number">0.6736</span></span>
<span class="line"><span class="number">0.7532</span></span>
<span class="line"><span class="number">0.7919</span></span>
<span class="line"><span class="number">0.8111</span></span>
<span class="line"><span class="number">0.8213</span></span>
<span class="line"><span class="number">0.8403</span></span>
<span class="line"><span class="number">0.8448</span></span>
<span class="line"><span class="number">0.8521</span></span>
<span class="line"><span class="number">0.856</span></span>
<span class="line"><span class="number">0.8613</span></span>
<span class="line"><span class="number">0.8649</span></span>
<span class="line"><span class="number">0.8573</span></span>
<span class="line"><span class="number">0.8691</span></span>
<span class="line"><span class="number">0.8694</span></span>
<span class="line"><span class="number">0.8738</span></span>
<span class="line"><span class="number">0.8741</span></span>
<span class="line"><span class="number">0.8745</span></span>
<span class="line"><span class="number">0.8796</span></span>
<span class="line"><span class="number">0.8821</span></span>
</pre></td></tr></table></figure></p>
<h1 id="Dropout-解决-overfitting"><a href="#Dropout-解决-overfitting" class="headerlink" title="Dropout 解决 overfitting"></a>Dropout 解决 overfitting</h1><p>Dropout是指在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了（有点抽象，具体实现看后面的实验部分）。<br>就是说：从输入层到第一层隐藏层训练后，drop掉第一层隐藏层中的一部分；从第一层隐藏层到第二层隐藏训训练后，drop掉第二层隐藏层中的一部分；……以此类推（看第二个例子<code>dropout net</code>那部分比较好理解）<br><strong>主要用于防止训练神经网络模型时，训练样本较少而导致的模型过拟合。</strong></p>
<h2 id="例子1-使用tensorboard观察"><a href="#例子1-使用tensorboard观察" class="headerlink" title="例子1 使用tensorboard观察"></a>例子1 使用tensorboard观察</h2><figure class="highlight python"><figcaption><span>TensorFlow 中 dropout 的使用</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span>
<span class="line">2</span>
<span class="line">3</span>
<span class="line">4</span>
<span class="line">5</span>
<span class="line">6</span>
<span class="line">7</span>
<span class="line">8</span>
<span class="line">9</span>
<span class="line">10</span>
<span class="line">11</span>
<span class="line">12</span>
<span class="line">13</span>
<span class="line">14</span>
<span class="line">15</span>
<span class="line">16</span>
<span class="line">17</span>
<span class="line">18</span>
<span class="line">19</span>
<span class="line">20</span>
<span class="line">21</span>
<span class="line">22</span>
<span class="line">23</span>
<span class="line">24</span>
<span class="line">25</span>
<span class="line">26</span>
<span class="line">27</span>
<span class="line">28</span>
<span class="line">29</span>
<span class="line">30</span>
<span class="line">31</span>
<span class="line">32</span>
<span class="line">33</span>
<span class="line">34</span>
<span class="line">35</span>
<span class="line">36</span>
<span class="line">37</span>
<span class="line">38</span>
<span class="line">39</span>
<span class="line">40</span>
<span class="line">41</span>
<span class="line">42</span>
<span class="line">43</span>
<span class="line">44</span>
<span class="line">45</span>
<span class="line">46</span>
<span class="line">47</span>
<span class="line">48</span>
<span class="line">49</span>
<span class="line">50</span>
<span class="line">51</span>
<span class="line">52</span>
<span class="line">53</span>
<span class="line">54</span>
<span class="line">55</span>
<span class="line">56</span>
<span class="line">57</span>
<span class="line">58</span>
<span class="line">59</span>
<span class="line">60</span>
<span class="line">61</span>
<span class="line">62</span>
<span class="line">63</span>
<span class="line">64</span>
</pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span>
<span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span>
<span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span>
<span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span>
<span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelBinarizer</span>
<span class="line"></span>
<span class="line"><span class="comment"># load data</span></span>
<span class="line">digits = load_digits()</span>
<span class="line">X = digits.data</span>
<span class="line">y = digits.target</span>
<span class="line">y = LabelBinarizer().fit_transform(y)</span>
<span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">.3</span>)</span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, layer_name, activation_function=None, )</span>:</span></span>
<span class="line">    <span class="comment"># add one more layer and return the output of this layer</span></span>
<span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span>
<span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>, )</span>
<span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span>
<span class="line">    <span class="comment"># here to dropout</span></span>
<span class="line">    Wx_plus_b = tf.nn.dropout(Wx_plus_b, keep_prob)</span>
<span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span>
<span class="line">        outputs = Wx_plus_b</span>
<span class="line">    <span class="keyword">else</span>:</span>
<span class="line">        outputs = activation_function(Wx_plus_b, )</span>
<span class="line">    tf.summary.histogram(layer_name + <span class="string">'/outputs'</span>, outputs)</span>
<span class="line">    <span class="keyword">return</span> outputs</span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="comment"># define placeholder for inputs to network</span></span>
<span class="line">keep_prob = tf.placeholder(tf.float32)          <span class="comment"># 百分比 保持多少的数据不被drop掉</span></span>
<span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">64</span>])     <span class="comment"># 8x8</span></span>
<span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span>
<span class="line"></span>
<span class="line"><span class="comment"># add output layer</span></span>
<span class="line">l1 = add_layer(xs, <span class="number">64</span>, <span class="number">50</span>, <span class="string">'l1'</span>, activation_function=tf.nn.tanh)</span>
<span class="line">prediction = add_layer(l1, <span class="number">50</span>, <span class="number">10</span>, <span class="string">'l2'</span>, activation_function=tf.nn.softmax)</span>
<span class="line"></span>
<span class="line"><span class="comment"># the loss between prediction and real data</span></span>
<span class="line"><span class="comment"># loss函数（即最优化目标函数）选用交叉熵函数。</span></span>
<span class="line"><span class="comment"># 交叉熵用来衡量预测值和真实值的相似程度，如果完全相同，交叉熵就等于零。</span></span>
<span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction),</span>
<span class="line">                                              reduction_indices=[<span class="number">1</span>]))  <span class="comment"># loss</span></span>
<span class="line">tf.summary.scalar(<span class="string">'loss'</span>, cross_entropy)</span>
<span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy)</span>
<span class="line"></span>
<span class="line">sess = tf.Session()</span>
<span class="line">merged = tf.summary.merge_all()</span>
<span class="line"><span class="comment"># summary writer goes in here</span></span>
<span class="line">train_writer = tf.summary.FileWriter(<span class="string">"../logs/train"</span>, sess.graph)</span>
<span class="line">test_writer = tf.summary.FileWriter(<span class="string">"../logs/test"</span>, sess.graph)</span>
<span class="line"></span>
<span class="line">init = tf.global_variables_initializer()</span>
<span class="line">sess.run(init)</span>
<span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">500</span>):</span>
<span class="line">    <span class="comment"># here to determine the keeping probability</span></span>
<span class="line">    sess.run(train_step, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">0.5</span>&#125;)      <span class="comment"># Dropout !!! 关键代码</span></span>
<span class="line">    <span class="comment"># keep_prob: 1 时    不dropout    可能会出现overfittinf</span></span>
<span class="line">    <span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span>
<span class="line">        <span class="comment"># record loss</span></span>
<span class="line">        train_result = sess.run(merged, feed_dict=&#123;xs: X_train, ys: y_train, keep_prob: <span class="number">1</span>&#125;)</span>
<span class="line">        test_result = sess.run(merged, feed_dict=&#123;xs: X_test, ys: y_test, keep_prob: <span class="number">1</span>&#125;)</span>
<span class="line">        train_writer.add_summary(train_result, i)</span>
<span class="line">        test_writer.add_summary(test_result, i)</span>
</pre></td></tr></table></figure>
<figure class="highlight cmd"><figcaption><span>在终端输入 </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span>
<span class="line">2</span>
<span class="line">3</span>
<span class="line">4</span>
<span class="line">5</span>
</pre></td><td class="code"><pre><span class="line">$ source ~/.bash_profile</span>
<span class="line">$ tensorboard --logdir="/Users/XuX/C-Code/P-Python/莫烦 机器学习/logs"</span>
<span class="line"></span>
<span class="line">Starting TensorBoard <span class="number">54</span> <span class="built_in">at</span> http://xushanshan.lan:<span class="number">6006</span></span>
<span class="line">(Press CTRL+C to quit)</span>
</pre></td></tr></table></figure>
<p>在阅览器输入地址<br>keep_prob=1时，出现过拟合现象：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-08-151118.jpg" alt=""><br>keep_prob=0.5时，好了很多：<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-08-142441.jpg" alt=""><br>可以看出两条曲线基本吻合，证明<strong>dropout后一定程度上避免了overfitting的现象</strong>。</p>
<h2 id="例子2-使用matplotlib观察"><a href="#例子2-使用matplotlib观察" class="headerlink" title="例子2 使用matplotlib观察"></a>例子2 使用matplotlib观察</h2><figure class="highlight python"><figcaption><span>TensorFlow 中 dropout 的使用 2</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span>
<span class="line">2</span>
<span class="line">3</span>
<span class="line">4</span>
<span class="line">5</span>
<span class="line">6</span>
<span class="line">7</span>
<span class="line">8</span>
<span class="line">9</span>
<span class="line">10</span>
<span class="line">11</span>
<span class="line">12</span>
<span class="line">13</span>
<span class="line">14</span>
<span class="line">15</span>
<span class="line">16</span>
<span class="line">17</span>
<span class="line">18</span>
<span class="line">19</span>
<span class="line">20</span>
<span class="line">21</span>
<span class="line">22</span>
<span class="line">23</span>
<span class="line">24</span>
<span class="line">25</span>
<span class="line">26</span>
<span class="line">27</span>
<span class="line">28</span>
<span class="line">29</span>
<span class="line">30</span>
<span class="line">31</span>
<span class="line">32</span>
<span class="line">33</span>
<span class="line">34</span>
<span class="line">35</span>
<span class="line">36</span>
<span class="line">37</span>
<span class="line">38</span>
<span class="line">39</span>
<span class="line">40</span>
<span class="line">41</span>
<span class="line">42</span>
<span class="line">43</span>
<span class="line">44</span>
<span class="line">45</span>
<span class="line">46</span>
<span class="line">47</span>
<span class="line">48</span>
<span class="line">49</span>
<span class="line">50</span>
<span class="line">51</span>
<span class="line">52</span>
<span class="line">53</span>
<span class="line">54</span>
<span class="line">55</span>
<span class="line">56</span>
<span class="line">57</span>
<span class="line">58</span>
<span class="line">59</span>
<span class="line">60</span>
<span class="line">61</span>
<span class="line">62</span>
<span class="line">63</span>
<span class="line">64</span>
<span class="line">65</span>
<span class="line">66</span>
<span class="line">67</span>
<span class="line">68</span>
<span class="line">69</span>
<span class="line">70</span>
</pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span>
<span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span>
<span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span>
<span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span>
<span class="line"></span>
<span class="line">tf.set_random_seed(<span class="number">1</span>)</span>
<span class="line">np.random.seed(<span class="number">1</span>)</span>
<span class="line"></span>
<span class="line"><span class="comment"># Hyper parameters</span></span>
<span class="line">N_SAMPLES = <span class="number">20</span></span>
<span class="line">N_HIDDEN = <span class="number">300</span></span>
<span class="line">LR = <span class="number">0.01</span></span>
<span class="line"></span>
<span class="line"><span class="comment"># training data</span></span>
<span class="line">x = np.linspace(<span class="number">-1</span>, <span class="number">1</span>, N_SAMPLES)[:, np.newaxis]</span>
<span class="line">y = x + <span class="number">0.3</span>*np.random.randn(N_SAMPLES)[:, np.newaxis]</span>
<span class="line"></span>
<span class="line"><span class="comment"># test data</span></span>
<span class="line">test_x = x.copy()</span>
<span class="line">test_y = test_x + <span class="number">0.3</span>*np.random.randn(N_SAMPLES)[:, np.newaxis]</span>
<span class="line"></span>
<span class="line"><span class="comment"># show data</span></span>
<span class="line">plt.scatter(x, y, c=<span class="string">'magenta'</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">'train'</span>)</span>
<span class="line">plt.scatter(test_x, test_y, c=<span class="string">'cyan'</span>, s=<span class="number">50</span>, alpha=<span class="number">0.5</span>, label=<span class="string">'test'</span>)</span>
<span class="line">plt.legend(loc=<span class="string">'upper left'</span>)</span>
<span class="line">plt.ylim((<span class="number">-2.5</span>, <span class="number">2.5</span>))</span>
<span class="line">plt.show()</span>
<span class="line"></span>
<span class="line"><span class="comment"># tf placeholders</span></span>
<span class="line">tf_x = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span>
<span class="line">tf_y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span>
<span class="line">tf_is_training = tf.placeholder(tf.bool, <span class="keyword">None</span>)  <span class="comment"># to control dropout when training and testing</span></span>
<span class="line"></span>
<span class="line"><span class="comment"># overfitting net</span></span>
<span class="line">o1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)</span>
<span class="line">o2 = tf.layers.dense(o1, N_HIDDEN, tf.nn.relu)</span>
<span class="line">o_out = tf.layers.dense(o2, <span class="number">1</span>)</span>
<span class="line">o_loss = tf.losses.mean_squared_error(tf_y, o_out)</span>
<span class="line">o_train = tf.train.AdamOptimizer(LR).minimize(o_loss)</span>
<span class="line"></span>
<span class="line"><span class="comment"># dropout net</span></span>
<span class="line">d1 = tf.layers.dense(tf_x, N_HIDDEN, tf.nn.relu)</span>
<span class="line">d1 = tf.layers.dropout(d1, rate=<span class="number">0.5</span>, training=tf_is_training)   <span class="comment"># drop out 50% of inputs</span></span>
<span class="line">d2 = tf.layers.dense(d1, N_HIDDEN, tf.nn.relu)</span>
<span class="line">d2 = tf.layers.dropout(d2, rate=<span class="number">0.5</span>, training=tf_is_training)   <span class="comment"># drop out 50% of inputs</span></span>
<span class="line">d_out = tf.layers.dense(d2, <span class="number">1</span>)</span>
<span class="line">d_loss = tf.losses.mean_squared_error(tf_y, d_out)</span>
<span class="line">d_train = tf.train.AdamOptimizer(LR).minimize(d_loss)</span>
<span class="line"></span>
<span class="line">sess = tf.Session()</span>
<span class="line">sess.run(tf.global_variables_initializer())</span>
<span class="line"></span>
<span class="line">plt.ion()   <span class="comment"># something about plotting</span></span>
<span class="line"></span>
<span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span>
<span class="line">    sess.run([o_train, d_train], &#123;tf_x: x, tf_y: y, tf_is_training: <span class="keyword">True</span>&#125;)  <span class="comment"># train, set is_training=True</span></span>
<span class="line"></span>
<span class="line">    <span class="keyword">if</span> t % <span class="number">10</span> == <span class="number">0</span>:</span>
<span class="line">        <span class="comment"># plotting</span></span>
<span class="line">        plt.cla()</span>
<span class="line">        o_loss_, d_loss_, o_out_, d_out_ = sess.run(</span>
<span class="line">            [o_loss, d_loss, o_out, d_out], &#123;tf_x: test_x, tf_y: test_y, tf_is_training: <span class="keyword">False</span>&#125; <span class="comment"># test, set is_training=False</span></span>
<span class="line">        )</span>
<span class="line">        plt.scatter(x, y, c=<span class="string">'magenta'</span>, s=<span class="number">50</span>, alpha=<span class="number">0.3</span>, label=<span class="string">'train'</span>); plt.scatter(test_x, test_y, c=<span class="string">'cyan'</span>, s=<span class="number">50</span>, alpha=<span class="number">0.3</span>, label=<span class="string">'test'</span>)</span>
<span class="line">        plt.plot(test_x, o_out_, <span class="string">'r-'</span>, lw=<span class="number">3</span>, label=<span class="string">'overfitting'</span>); plt.plot(test_x, d_out_, <span class="string">'b--'</span>, lw=<span class="number">3</span>, label=<span class="string">'dropout(50%)'</span>)</span>
<span class="line">        plt.text(<span class="number">0</span>, <span class="number">-1.2</span>, <span class="string">'overfitting loss=%.4f'</span> % o_loss_, fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>:  <span class="string">'red'</span>&#125;); plt.text(<span class="number">0</span>, <span class="number">-1.5</span>, <span class="string">'dropout loss=%.4f'</span> % d_loss_, fontdict=&#123;<span class="string">'size'</span>: <span class="number">20</span>, <span class="string">'color'</span>: <span class="string">'blue'</span>&#125;)</span>
<span class="line">        plt.legend(loc=<span class="string">'upper left'</span>); plt.ylim((<span class="number">-2.5</span>, <span class="number">2.5</span>)); plt.pause(<span class="number">0.1</span>)</span>
<span class="line"></span>
<span class="line">plt.ioff()</span>
<span class="line">plt.show()</span>
</pre></td></tr></table></figure>
<p>动态的观察是否添加dropout的区别<br><img src="http://onk12tr6m.bkt.clouddn.com/2017-08-09-044404.jpg" alt=""></p>
<hr>
<p><strong>参考资料</strong><br><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/" target="_blank" rel="external">TensorFlow 官方文档中文版</a><br><a href="https://morvanzhou.github.io/tutorials/" target="_blank" rel="external">莫烦Python</a><br><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/" target="_blank" rel="external">神经网络：Tensorflow</a></p>
</div><div class="tags"><a href="/tags/神经网络/">神经网络</a><a href="/tags/tensorflow/">tensorflow</a><a href="/tags/dropout/">dropout</a></div><div class="post-share"><div class="bdsharebuttonbox"><span style="float:left;line-height: 28px;height: 28px;font-size:16px;font-weight:blod">分享到：</span><a href="#" data-cmd="more" class="bds_more"></a><a href="#" data-cmd="mshare" title="分享到一键分享" class="bds_mshare"></a><a href="#" data-cmd="fbook" title="分享到Facebook" class="bds_fbook"></a><a href="#" data-cmd="twi" title="分享到Twitter" class="bds_twi"></a><a href="#" data-cmd="linkedin" title="分享到linkedin" class="bds_linkedin"></a><a href="#" data-cmd="youdao" title="分享到有道云笔记" class="bds_youdao"></a><a href="#" data-cmd="evernotecn" title="分享到印象笔记" class="bds_evernotecn"></a><a href="#" data-cmd="weixin" title="分享到微信" class="bds_weixin"></a><a href="#" data-cmd="qzone" title="分享到QQ空间" class="bds_qzone"></a><a href="#" data-cmd="tsina" title="分享到新浪微博" class="bds_tsina"></a></div></div><div class="post-nav"><a href="/2017/07/05/Tensorflow4/" class="pre">【神经网络】通过代码学习Tensorflow4</a><a href="/2017/07/02/Tensorflow2/" class="next">【神经网络】通过代码学习Tensorflow2</a></div><div id="comments"></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Classification-分类学习"><span class="toc-text">Classification 分类学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dropout-解决-overfitting"><span class="toc-text">Dropout 解决 overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#例子1-使用tensorboard观察"><span class="toc-text">例子1 使用tensorboard观察</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#例子2-使用matplotlib观察"><span class="toc-text">例子2 使用matplotlib观察</span></a></li></ol></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/08/16/正则表达/">正则表达式</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/13/PyTorch/">【神经网络】通过代码学习PyTorch</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/11/Tensorflow6/">【神经网络】通过代码学习Tensorflow6</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/11/Deep-Learning/">【深度学习】深度神经网络入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/05/有趣的机器学习/">【机器学习】有趣的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/04/系统中多版本Python的切换与Python第三方库的安装/">系统中多版本Python的切换与Python第三方库的安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/22/位运算/">位运算</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/剑指offer-31-40/">剑指offer 31~40</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/21/vector的初始化/">【C++】vector介绍</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/19/stringstream/">【C++】stringstream的使用方法</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/C-笔记/">C++笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术文档/">技术文档</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习笔记/">机器学习笔记</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/杂记/">杂记</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习笔记/">深度学习笔记</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法学习笔记/">算法学习笔记</a><span class="category-list-count">12</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/梯度下降/" style="font-size: 15px;">梯度下降</a> <a href="/tags/凸包/" style="font-size: 15px;">凸包</a> <a href="/tags/基本概念/" style="font-size: 15px;">基本概念</a> <a href="/tags/近似算法/" style="font-size: 15px;">近似算法</a> <a href="/tags/数学/" style="font-size: 15px;">数学</a> <a href="/tags/概率论/" style="font-size: 15px;">概率论</a> <a href="/tags/线性代数/" style="font-size: 15px;">线性代数</a> <a href="/tags/数值计算/" style="font-size: 15px;">数值计算</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/权值衰减/" style="font-size: 15px;">权值衰减</a> <a href="/tags/范数/" style="font-size: 15px;">范数</a> <a href="/tags/分治算法/" style="font-size: 15px;">分治算法</a> <a href="/tags/RBM/" style="font-size: 15px;">RBM</a> <a href="/tags/DBN/" style="font-size: 15px;">DBN</a> <a href="/tags/CNN/" style="font-size: 15px;">CNN</a> <a href="/tags/RNN/" style="font-size: 15px;">RNN</a> <a href="/tags/Autoencoder/" style="font-size: 15px;">Autoencoder</a> <a href="/tags/RNTN/" style="font-size: 15px;">RNTN</a> <a href="/tags/动态规划/" style="font-size: 15px;">动态规划</a> <a href="/tags/贪心/" style="font-size: 15px;">贪心</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/数学基础/" style="font-size: 15px;">数学基础</a> <a href="/tags/MathJax/" style="font-size: 15px;">MathJax</a> <a href="/tags/LaTex/" style="font-size: 15px;">LaTex</a> <a href="/tags/Matplotlib/" style="font-size: 15px;">Matplotlib</a> <a href="/tags/Markdown/" style="font-size: 15px;">Markdown</a> <a href="/tags/Hexo/" style="font-size: 15px;">Hexo</a> <a href="/tags/感知机/" style="font-size: 15px;">感知机</a> <a href="/tags/sigmoid神经元/" style="font-size: 15px;">sigmoid神经元</a> <a href="/tags/神经网络/" style="font-size: 15px;">神经网络</a> <a href="/tags/Graham-s-Scan/" style="font-size: 15px;">Graham's Scan</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/数据处理/" style="font-size: 15px;">数据处理</a> <a href="/tags/Panadas/" style="font-size: 15px;">Panadas</a> <a href="/tags/信息学/" style="font-size: 15px;">信息学</a> <a href="/tags/PyTorch/" style="font-size: 15px;">PyTorch</a> <a href="/tags/随机算法/" style="font-size: 15px;">随机算法</a> <a href="/tags/sklearn/" style="font-size: 15px;">sklearn</a> <a href="/tags/tensorflow/" style="font-size: 15px;">tensorflow</a> <a href="/tags/dropout/" style="font-size: 15px;">dropout</a> <a href="/tags/tensorboard/" style="font-size: 15px;">tensorboard</a> <a href="/tags/搜索/" style="font-size: 15px;">搜索</a> <a href="/tags/VC维/" style="font-size: 15px;">VC维</a> <a href="/tags/VPN/" style="font-size: 15px;">VPN</a> <a href="/tags/字符串/" style="font-size: 15px;">字符串</a> <a href="/tags/堆/" style="font-size: 15px;">堆</a> <a href="/tags/STL/" style="font-size: 15px;">STL</a> <a href="/tags/vector/" style="font-size: 15px;">vector</a> <a href="/tags/数组/" style="font-size: 15px;">数组</a> <a href="/tags/位运算/" style="font-size: 15px;">位运算</a> <a href="/tags/链表/" style="font-size: 15px;">链表</a> <a href="/tags/递归和循环/" style="font-size: 15px;">递归和循环</a> <a href="/tags/代码的完整性/" style="font-size: 15px;">代码的完整性</a> <a href="/tags/代码的鲁棒性/" style="font-size: 15px;">代码的鲁棒性</a> <a href="/tags/抽象具体化/" style="font-size: 15px;">抽象具体化</a> <a href="/tags/举例让抽象具体化/" style="font-size: 15px;">举例让抽象具体化</a> <a href="/tags/分解让复杂问题简单/" style="font-size: 15px;">分解让复杂问题简单</a> <a href="/tags/时间效率/" style="font-size: 15px;">时间效率</a> <a href="/tags/指针/" style="font-size: 15px;">指针</a> <a href="/tags/正则表达式/" style="font-size: 15px;">正则表达式</a> <a href="/tags/Python/" style="font-size: 15px;">Python</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">六月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">五月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">四月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/03/">三月 2017</a></li></ul></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p><a href="/baidusitemap.xml">Baidu Site Haritası</a> |  <a href="/atom.xml">订阅</a> |  <a href="/about/">关于</a></p><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">Xu Shanshan.</a></span><span> Theme by<a rel="nofollow" target="_blank" href="https://github.com/chaooo/hexo-theme-BlueLake"> BlueLake.</a></span><span> Count by<a href="http://busuanzi.ibruce.info/"> busuanzi.</a></span><span> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":["mshare","weixin","tsina","qzone","linkedin","fbook","twi","print","renren","sqq","evernotecn","bdysc","tqq","tqf","bdxc","kaixin001","tieba","douban","bdhome","thx","ibaidu","meilishuo","mogujie","diandian","huaban","duitang","hx","fx","youdao","sdo","qingbiji","people","xinhua","mail","isohu","yaolan","wealink","ty","iguba","h163","copy"],"bdPic":"","bdStyle":"1","bdSize":"16"},"share":{},"image":{"viewList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"],"viewText":"分享到：","viewSize":"16"},"selectShare":{"bdContainerClass":null,"bdSelectMiniList":["tsina","qzone","weixin","fbook","twi","linkedin","youdao","evernotecn","mshare"]}};with(document)0[(getElementsByTagName('head')[0]||head).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];
</script></body></html>